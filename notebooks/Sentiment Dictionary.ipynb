{
 "metadata": {
  "name": "",
  "signature": "sha256:df695d02835357e8ee3f1b470298fa81238b083d8940531e96b91baf85a55c22"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook chronicles my efforts to build a sentiment dictionary using the existing lexicon in the yelp reviews."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Option 1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. Select only 1-star and 5-star reviews.\n",
      "2. Assume that all words in 1-star reviews are bad. Assume that all words in 5-star reviews are good.\n",
      "3. Calculate tf-idf score of each word in the 5-star reviews.\n",
      "4. Take words that are in upper part of tf-idf distribution (but not the highest)\n",
      "5. Screen for part of speech (use adjectives, nouns, adverbs, other?)\n",
      "6. These are your \"good\" and \"bad\" sentiment words. assign them values (many options here)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Option 2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. Select all reviews\n",
      "2. Select all words within N words of a known, previously labeled sentiment word (\"amazing\",\"terrible\",\"gross\",\"excellent\")\n",
      "3. do the tf-idf calculation and screening on those words."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Option 3+"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "use pre-existing sentiment dictionaries"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "1. Build lexicon from 1 and 5 star reviews"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "review_data = pd.read_pickle('../data/pandas/review_mexican.pkl')\n",
      "onestar = review_data[review_data['stars']==1]\n",
      "fivestar = review_data[review_data['stars']==5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "stem and label word parts of speech"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This code borrows very heavily, and often copies from:\n",
      "# http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "import external.potts_tokenizer as potts\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "\n",
      "def stem_tokens(tokens,stemmer):\n",
      "    \"\"\"stems a list of tokens using the given stemmer.\"\"\"\n",
      "    stemmed = []\n",
      "    for token in tokens:\n",
      "        stemmed.append(stemmer(token))\n",
      "    return stemmed\n",
      "\n",
      "def tokenize(text):\n",
      "    \"\"\"takes raw text of review and prepares it for use in the sklearn.tfidf\n",
      "        This consists of:\n",
      "            1. tokenizing words with the potts sentiment tokenizer\n",
      "            2. stemming english words using the Porter Stemmer\n",
      "    \"\"\"\n",
      "\n",
      "    stemmer = PorterStemmer()\n",
      "    tokens = potts.Tokenizer(text)\n",
      "    return stem_tokens(tokens, stemmer)\n",
      "\n",
      "    \n",
      "onedict = {}\n",
      "for item in onestar.index:\n",
      "    review = onestar[item]\n",
      "    onedict[review.review_id] = processed(review.text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named external.potts_tokenizer",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-5-a0b5851d0207>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mexternal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpotts_tokenizer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpotts_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mImportError\u001b[0m: No module named external.potts_tokenizer"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf = sklearn.TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
      "tfs = tfidf.fit_transform(onedict.values())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}