{
 "metadata": {
  "name": "",
  "signature": "sha256:237b34a73334d26f4a60dccb4fb3cda3f06bba37994f99f685f02562a27a998d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Build Training Data for sentiment scoring as well as static aspect identification."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import random\n",
      "import nltk\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "1. Select data for training"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load restaurant data\n",
      "rest = pd.read_pickle('data/restaurant_data_mexican.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Randomly choose 20 restaurants from the dataset for this process\n",
      "random.seed(1234)\n",
      "trainids = random.sample(rest.business_id,20)\n",
      "trainrest = rest[rest.business_id.isin(trainids)]\n",
      "#print rest[rest.business_id.isin(trainids)].review_count.mean()\n",
      "#print rest.review_count.mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "41.45\n",
        "46.8013544018\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get reviews associated with these restaurants\n",
      "rev = pd.read_pickle('data/review_data_mexican.pkl')\n",
      "trainrev = rev[rev.business_id.isin(trainids)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Divide reviews into individual sentences\n",
      "# Make new table that has: Review_ID+Sentece_ID, sentence \n",
      "sentences = pd.DataFrame(columns=['review_id','sentence_id','text','stars'])\n",
      "total_count = 0\n",
      "for rev_id in trainrev.review_id:\n",
      "    thisreview = trainrev[trainrev.review_id==rev_id]\n",
      "    sents = nltk.sent_tokenize(thisreview.text.values[0])\n",
      "    thissent_count = 0\n",
      "    for sent in sents:\n",
      "        sentid = str(thissent_count).zfill(5)\n",
      "        sentences.loc[total_count] = [thisreview.review_id.values[0],sentid,sent,thisreview.stars.values[0]]\n",
      "        thissent_count += 1\n",
      "        total_count += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentences.to_pickle('data/review_sentences_mexican.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>review_id</th>\n",
        "      <th>sentence_id</th>\n",
        "      <th>text</th>\n",
        "      <th>stars</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 9qt9WJuegXQKgqVogRGxnQ</td>\n",
        "      <td> 00000</td>\n",
        "      <td> Had lunch at this location for the first time ...</td>\n",
        "      <td> 4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 9qt9WJuegXQKgqVogRGxnQ</td>\n",
        "      <td> 00001</td>\n",
        "      <td> I had been to the other location in Scottsdale...</td>\n",
        "      <td> 4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 9qt9WJuegXQKgqVogRGxnQ</td>\n",
        "      <td> 00002</td>\n",
        "      <td> many times, with mixed, yet overall favorable ...</td>\n",
        "      <td> 4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 9qt9WJuegXQKgqVogRGxnQ</td>\n",
        "      <td> 00003</td>\n",
        "      <td> While it must be stated that this isn't going ...</td>\n",
        "      <td> 4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 9qt9WJuegXQKgqVogRGxnQ</td>\n",
        "      <td> 00004</td>\n",
        "      <td> I had a machaca burrito, enchilada style for $...</td>\n",
        "      <td> 4</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 95,
       "text": [
        "                review_id sentence_id  \\\n",
        "0  9qt9WJuegXQKgqVogRGxnQ       00000   \n",
        "1  9qt9WJuegXQKgqVogRGxnQ       00001   \n",
        "2  9qt9WJuegXQKgqVogRGxnQ       00002   \n",
        "3  9qt9WJuegXQKgqVogRGxnQ       00003   \n",
        "4  9qt9WJuegXQKgqVogRGxnQ       00004   \n",
        "\n",
        "                                                text  stars  \n",
        "0  Had lunch at this location for the first time ...      4  \n",
        "1  I had been to the other location in Scottsdale...      4  \n",
        "2  many times, with mixed, yet overall favorable ...      4  \n",
        "3  While it must be stated that this isn't going ...      4  \n",
        "4  I had a machaca burrito, enchilada style for $...      4  "
       ]
      }
     ],
     "prompt_number": 95
    }
   ],
   "metadata": {}
  }
 ]
}